\documentclass{book}
\usepackage{hevea}

\def\t#1{{\tt #1}}
\def\DYNAMIC{\t{DYNAMIC}}
\def\secref#1{Section~\ref{sec-#1}}
\def\chref#1{Chapter~\ref{ch-#1}}

\title{CCured: User Manual}

% Make sure that all documents show up in the main frame
%HEVEA \AtBeginDocument{\@print{<base target="main">}}

\begin{document}
\maketitle
\chapter{Introduction}

CCured is a source-to-source translator for C, which instruments the source C
code with runtime checks to catch memory safety violations at run time. As a
novel enhancement, the system infers when some (or all!) of the checks can be
safely omitted.

 The translator itself is written in Ocaml (a dialect of ML).  There is
also a Perl script, ccured.pl, which functions as a drop-in
replacement for 'cc', so that software packages' existing Makefiles
can be used unmodified.  Finally, various runtime functions (including
the Boehm-Weiser conservative garbage collector) are provided by C
code which we write and is linked with the translated executable. If you want
to look at the implementation of CCured then you might want to learn first
about the C Intermediate Language (\ahrefurl{cil/index.html}) that CCured is using:
 

 
\chapter{Using CCured}

 CCured consists of several components: an Ocaml application that does the
main work, a set of Perl scripts that are used to invoke the CCured
application, and a set of header and run-time library files. 

 To use CCured you should put the \t{lib} directory in your \t{PATH} (see
\chref{start}) and then you can invoke the CCured wrapper as follows:
\begin{verbatim}
perl -S ccured.pl [options]
\end{verbatim}

 (the \t{-S} option instructs \t{perl} to find the \t{ccured.pl} script in the
 \t{PATH}). 

 The \t{ccured.pl} script is meant to behave as a drop-in replacement for
\t{gcc} and Microsoft Visual C compiler. This means that you can use CCured
with your existing Makefile, just make sure to set the \t{CC} variable to the
above command:
\begin{verbatim}
make mystuff CC="perl -S ccured.pl [options]"
\end{verbatim}

 {\bf IMPORTANT:} If you want to use CCured instead of the Microsoft Visual C
compiler, the first argument for the \t{ccured.pl} script must
\t{"--mode=mscl"}. 

 Among the options for the \t{ccured.pl} you can put anything that can
normally go in the command line of the compiler that CCured is impersonating.
\t{ccured.pl} will do its best to pass those options along to the appropriate
subprocess. In addition, the following options are supported:

\begin{itemize}
\item \t{--help} Prints a list of the options supported.
\item \t{--debug} Use the debug version of the CCured application. By default
the release version is used.
\item \t{--verbose} Prints lots of messages about what is going on.
\item \t{--combine} This tells CCured to first attempt to collect into one
source file all of the sources that make your application, and then to apply
CCured on the resulting source. To see how this is accomplished, see
\chref{combiner}. This option is normally used when you use CCured with a
Makefile.
\item \t{--nocure=xxx}. Do not cure the files whose basename is "xxx". This is
used in combination with the \t{--combine} option.
\item \t{--includedir=xxx}. Override the include files with those in the given
directory. The given directory is the same name that was given an an argument
to the patcher (see Chapter \chref{patcher}). In particular this means that
that directory contains subdirectories named based on the current compiler
version. The patcher creates those directories. 
\item \t{--usecabs}. Do not cure, but instead just parse the source and print
its AST out. 
\item \t{--usecil}. Do not cure, but instead just parse the source, convert to
CIL and then print it out. 
\item \t{--curetype=xxx}. Specify what kind of cure is desired. Valid options
currently are:
   \begin{itemize}
    \item \t{none}. No cure. This is the default. In that case the
           \t{--includedir} and the \t{--nocure} options have no effect. 
    \item \t{infer}. This is the regular cure. Use the inferencer.
    \item \t{wild}. This makes all pointers ma\DYNAMIC{}.
   \end{itemize}
\item \t{--emitinfer}. This means that the result of inference is also
printed. 
\item \t{--optimize}. Run the optimizer after curing.
\item \t{--stats}. Print some statistics about the inserted checks.
\item \t{--nogc}. Will assume that you do not use a garbage collector. In this
case the cured code will use explicit deallocation and might be unsafe.
\end{itemize}

 All of the other options that start with \t{-} (and are not recognized as
compiler options) are passed unmodified to the CCured OCaml application. Among
those options the following might be of interest:
\begin{itemize}
\item \t{--check}. Run a consistency check over the CIL after every operation. 
\item \t{--logCalls}. Insert code in the processed source to print the name of
funtions as are called. 
\item \t{--log=xxx}. Set \t{xxx} to be the name of the log file for the CCured
application. By default \t{stderr} is used.
\item \t{--stages}. Print the stages that CCured is performing.
\item \t{--keepunused}. Do not attempt to remove the unused variables and
types from CIL. 
\item \t{--noPrintLn}. Do not print line numbers.
\item \t{--commPrintLn}. Print line numbers but in comments.
\item \t{--optimonly}. The source file is already cured. Invoke just the
                      optimizer for it.
\end{itemize}

 For example you can run the following commands:
\begin{verbatim}
perl -S ccured.pl test/small1/hello.c -o hello
perl -S ccured.pl --curetype=infer test/small1/hello.c -o hello
\end{verbatim}

 \section{Using the {\tt make} interface}

 While it is possible to use CCured using just the \t{ccured.pl} script we
have found it easier to write a \t{Makefile} that contains invocations of
\t{ccured.pl}. See \chref{start} for how to configure the
\t{Makefile}. 
 
 To use the \t{make} interface you use the following command:
\begin{verbatim}
make target [options]
\end{verbatim}

 The following targets are currently recognized:
\begin{itemize}
\item \t{setup}. This makes the undrlying CCured application and prepares the
patched include files.
\item \t{test/xxx}.  This will run CCured in compile-only mode on the single
file \t{test/small1/xxx.c}. 
\item \t{testrun/xxx}. This is like the above but will make an executable and
will then run the result. 
\item \t{bh}, \t{bisort}, \t{tsp}, etc. These are a number of test cases that
we have loaded in the \t{test} directory. The complete list of these can be
obtained by reading the \t{Makefile}.
\end{itemize}

 To customize the operation of CCured you can pass the following options to
the \t{make} command:
\begin{itemize}
\item \t{\_GNUCC=1}. This will make sure that you run in \t{gcc} mode even if
your configuration is for Microsoft Visual C. This has no effect on Linux.
\item \t{RELEASE=1}. To make sure you run the optimized executables. Do
\t{make setup RELEASE=1} to build the executables first.
\item \t{VERBOSE=1}. Passes the \t{--verbose} flag to CCured.
\item \t{PRINTSTAGES=1}. Passes the \t{--stages} flag to CCured.
\item \t{NOREMAKE=1}. Does not attempt to check the freshness and to make the
underlying CCured application. This is useful in test cases that you want to
run while working on the CCured sources.
\item \t{INFERBOX=xxx}. Passes the \t{--curetype=xxx} flag to CCured. Use
either \t{infer} or \t{wild} as \t{xxx}.
\item \t{USECABS=1}. Passes \t{--usecabs} to CCured.
\item \t{USECIL=1}. Passes \t{--usecil} to CCured.
\item \t{OPTIM=1}. Passes \t{--optimize} to CCured.
\item \t{STATS=1}. Passes \t{--stats} to CCured.
\item \t{NOGC=1}. Passes \t{--nogc} to CCured.
\item \t{EXTRARGS="..."}. Passes \t{...} to CCured.
\end{itemize}

 If you don't specify any options, the translator operates in 'cil' mode:
it reads the file in, translates it into our intermediate format, and
then spits it right back out (no instrumentation).  This is primarily a
test of the parser and the intermediate format.

To add instrumentation, use "INFERBOX=infer" or "INFERBOX=wild".  The former
uses type inference to try to remove many of the runtime checks.  The
latter does not try to remove any checks.  Generally, "INFERBOX=infer" is
what you want:

\begin{verbatim}
  make bh INFERBOX=infer
\end{verbatim}


\chapter{Using CCured on new software packages}

TODO: Add some info about putting new packages into the repository.

Once a package is in the repository and has its own Makefile target, you can
run the translator on it. To create a Makefile target for a package you
typically add to cil/Makefile a target that just invokes make on the package's
own Makefile with the CC variable bound to ``perl ccured.pl --combine''. I'll
use the 6/29/01 version of ftpd as my example.

First we try to run it without our tool involved:

\begin{verbatim}
  % make ftpd-clean
  % cd test/ftpd/ftpd
  % make
\end{verbatim}
  
This succeeds, generating an 'ftpd' binary.  In the case of 'ftpd', 
running it is slightly complicated:

\begin{verbatim}
  % ./ftpd -D -d -p 3333
  (then in another window)
  % telnet localhost 3333
  Trying 127.0.0.1...
  Connected to localhost.
  Escape character is '^]'.
  220 madrone.cs.berkeley.edu FTP server (Version 6.5/OpenBSD, linux port 0.3.2) ready.
  (etc)
\end{verbatim}
  
This is (to our way of measuring) success.

Now we try it in 'cil' mode:

\begin{verbatim}
  % cd cil
  % make ftpd-clean
  % make ftpd
\end{verbatim}
  
At the moment, this also works, producing another 'ftpd' binary.  We test
it the same way, and rejoice at its success.

Finally, we dare to try it in 'box' mode, meaning the instrumentation
module will be used:

\begin{verbatim}
  % make ftpd-clean
  % make ftpd INFERBOX=4
\end{verbatim}
  
After crunching for a while, it reports this error (you have to scroll
back a bit to see the right one):

\begin{verbatim}
  ./ls_all.c:1338: Bug: Calling non-wild ioctl with too many args
\end{verbatim}

This is an error from the 'box' module, complaining about what it
perceives to be a type error.  If we investigate the named source
line, we see

\begin{verbatim}
  if(ioctl(1, 0x5413, & win) == 0 && win.ws_col > 0)
\end{verbatim}

confirming that 'ioctl' is involved.  Since the \t{*\_all.c} files are the
output of our tool, and do not themselves \t{\#include} any files, we can
simply search in this file for ioctl's declaration.  We do so, and see

\begin{verbatim}
  extern int ioctl(int __fd , unsigned long int __request , ...) ;
\end{verbatim}
  
Hmmm... looks like it was declared to accept any number (>=2) of args, so
this looks like a bug in the 'box' module; it should accept this code,
but it does not.

The next step is to write a tiny C program which calls ioctl (see
test/small2/ioctl.c), and verify it fails the same way

\begin{verbatim}
  % make scott/ioctl INFERBOX=4
  [...]
  ioctl.c:9: Bug: Calling non-wild ioctl with too many args
  [...]
\end{verbatim}
  
Yep, same problem.  Now we report this to George, since typically he's
much faster at identifying the problem, since he wrote the 'box' module.

In the meantime (waiting for George to magically fix the problem), we
could temporarily comment-out the call so we can proceed to find other
bugs.  Or, perhaps we change the ioctl call to instead call a wrapper
function (wrappers are defined in lib/safec.c, which gets linked into
the translated program).

Eventually (see "make go INFERBOX=4") we'll get an executable.  If it
runs correctly, celebration is in order.  If not, it will usually fail
because of a failed runtime check (this one is from a test vector for
which go fails);

\begin{verbatim}
  % make go INFERBOX=4
  % cd test/spec/099.go/src
  % ./go 5 4
  [...]
  array bug: index is 5980 (vs 5980)
  Failure: Ubound
  Abort
\end{verbatim}

Tracking down the source of such failures is the most timeconsuming
part of pushing a program through.  Sometimes it's a bug in the
translator, in which case ideally a test case can be isolated for easy
diagnosis.  

Sometimes (more and more often) it's a bug in the original program (go
had 10 array bounds violation bugs at last count).  In this case you
have to change the original code to fix the bug; this may be easy or
hard.  If it's hard, try just surrounding the offending statement with
an explicit bounds check in an 'if' statement, so the program skips
the bad statements (that is what I did to cause all of the "array bug"
outputs in the "5 4" case above).

\chapter{Using CCured on Apache Modules}

\section{Introduction}

This writing assumes Apache 1.3.19 and an x86/Linux system. Apache is an
open source web server that has the ability to dynamically load third-party
modules. Modules can examine and alter HTTP requests and also examine and
alter the webserver replies. For example, a compression module might
examine the HTTP request to see if it contains the ``Accept-Encoding:
gzip'' tag. If it does it might alter the HTTP reply, replacing the text of
the webpage body with a compressed version of that text. Modules can be
configured (via a file called {\tt httpd.conf}) so that their behavior is
limited to a certain location or directory. 

Apache modules share the same address space as the Apache webserver: no
software fault isolation is present. As a result, if the module crashes it
brings down that webserver (although apache is usually configured to
immediately spawn a new webserver thread to replace the fallen one). More
distressingly, a module with a security violation (for example, a
format-string bug) can allow remote users to gain shell access to the
webserver machine (one version of {\tt mod\_php3} features such a
vulnerability: CCured prevents that vulnerability). 

Apache modules are typically single files with a fairly standard naming
convention: {\tt mod\_foo.c} is the {\tt foo} module, where {\tt foo}
ranges over fairly descriptive keywords like {\tt gzip}, {\tt random}, {\tt
urlcount}, {\tt auth} or {\tt layout}. {\tt mod\_foo.c} almost invariable
contains a global data structure of type {\tt module} with the C name {\tt
foo\_module}. This data structure is a table of function pointers and entry
points. Once {\tt mod\_foo.c} has been compiled to the shared object {\tt
mod\_foo.so}, Apache will dynamically load it and call the function
pointers listed in the {\tt foo\_module} structure at the appropriate time
(e.g., when a new request comes in). 

\section{Curing Apache Modules}

Most Apache modules are of a relatively modest size and curing them is no
great chore. However, some annotation work must be done. Since the cured
module must interact with the non-cured Apache webserver, objects that are
passed between them must not change size. As a result, {\tt WILD} and other
fat pointers cannot be introduced into such objects. Annotations must be
added to convince CCured that the module can be made safe without such
run-time checks. 

Imagine that you are trying to cure {\tt mod\_urlcount.c}.  Take out your
favorite text editor and open up the file. Near the top you should find a
configuration record structure. Each module defines a separate
configuration record structure with a separate (non-exported) name. For
example, {\tt mod\_urlcount} has:

\begin{verbatim}
typedef struct urlcount_config_rec {
    int         urlcount_default;
    CounterType urlcount_type;
    int         urlcount_auto_add;
    char       *urlcount_file;
} urlcount_config_rec;
\end{verbatim}

Each module also contains functions that create, manipulate and merge such
configuration structures. This is the mechanism through which Apache modules
maintain global state. Each time Apache calls one of the function pointers
exported by the module, it passes along a way to get to the appropriate
configuration record. Since Apache does not know how the config structure
will be defined, it uses {\tt void} pointers to describe the type. CCured
comes with a set of macros that instantiate those {\tt void *}s on a
per-module basis. Add the line:

\begin{verbatim}
NEW_MODULE_TYPE(urlcount, urlcount_config_rec)  // this is a macro
\end{verbatim}

where the first parameter is the module suffix name and the second is the
type name of the configuration record. This macro declares a type named
{\tt module\_urlcount}.  As mentioned earlier, each module exports a module
structure (full of function pointers). We must redeclare the module to take
advantage of the instantiated types. Change:

\begin{verbatim}
module urlcount_module; // full of "void *"s
\end{verbatim}

to

\begin{verbatim}
module_urlcount urlcount_module; // uses "url_config_rec *", not "void *"
\end{verbatim}

Now scroll down a bit and look for the word keyword {\tt void}. Apache
modules often feature unnecessary casts to void. For example, {\tt
mod\_headers} contains this function:

\begin{verbatim}
static void *merge_headers_config(pool *p, void *basev, void *overridesv)
{
    headers_conf *a = (headers_conf *) ap_pcalloc(p, sizeof(headers_conf));
    headers_conf *base = (headers_conf *) basev, 
        *overrides = (headers_conf *) overridesv;
    a->headers = ap_append_arrays(p, base->headers, overrides->headers);
    return a;
}
\end{verbatim}

Every {\tt void} in this function really stands for {\tt headrs\_conf} (the
{\tt mod\_headers} version of {\tt urlcount\_config\_rec}). Change it so
that the {\tt void} types are no longer present: 

\begin{verbatim}
static headers_conf *
    merge_headers_config(pool *p, headers_conf *basev, headers_conf *overridesv)
{
    headers_conf *a = (headers_conf *) ap_pcalloc(p, sizeof(headers_conf));
    headers_conf *base = (headers_conf *) basev, 
        *overrides = (headers_conf *) overridesv;
    a->headers = ap_append_arrays(p, base->headers, overrides->headers);
    return a;
}
\end{verbatim}

Repeat this process with all configuration functions that contain {\tt
void}. Now search for {\tt ap\_get\_module\_config}. It is a macro that
contains a (safe) cast to and from {\tt void *} -- it allows modules to
extract their configuration record from the global server state. For
example, {\tt mod\_headers} contains:

\begin{verbatim}
headers_conf *serverconf =
    (headers_conf *) ap_get_module_config(s->module_config, &headers_module);
\end{verbatim}

Change this to:

\begin{verbatim}
    headers_conf * serverconf;
    { __NOBOXBLOCK
    serverconf = ap_get_module_config(s->module_config, &headers_module);
    } 
\end{verbatim}

The {\tt \_\_NOBOXBLOCK} block keyword tells CCured to leave the block
alone: we are asserting that it is already safe. Modify every instance of
{\tt ap\_get\_module\_config} and {\tt ap\_get\_perdir\_module\_config}
the same way. 

Now look for a datatype with the suffix {\tt entry}. For example, {\tt
mod\_headers} features:

\begin{verbatim}
typedef struct {
    hdr_actions action;
    char *header;
    char *value;
} header_entry;
\end{verbatim}

This marks a use of Apache's polymorphic (via {\tt void *}) array routines. 
Insert the following macro declaration to tell CCured about this array
type:

\begin{verbatim}
NEW_TABLE_TYPE(header_entry, header_entry)      // macro
\end{verbatim}

This macro declares a new type, {\tt array\_header\_FOO} (where {\tt FOO}
is the first argument) that is a specialized version of the Apache-provided
type {\tt array\_header}. Other data structure (for example, the
configuration record structure) will contain {\tt array\_header}s. We
change them to use this new datatype. Change all declarations like:

\begin{verbatim}
typedef struct {
    array_header *headers;
} headers_conf;
\end{verbatim}

into:

\begin{verbatim}
typedef struct {
    array_header_header_entry *headers;
} headers_conf;
\end{verbatim}

Now search for every call to {\tt ap\_make\_array}, {\tt
ap\_append\_arrays}, {\tt ap\_push\_array} and append {\tt FOO} (in our
runing example, {\tt header\_entry}) to the name of each called function.
For example, change: 

\begin{verbatim}
new = (header_entry *) ap_push_array(dirconf->headers);
\end{verbatim}

into:

\begin{verbatim}
new = (header_entry *) ap_push_array_header_entry(dirconf->headers);
\end{verbatim}

Finally, surround all global table declarations with {\tt \#pragma}s that
tell CCured to leave them alone (because Apache must read them). Often
there are three such global tables per module. One is an array of {\tt
struct const command\_rec}s, one is an array of {\tt struct const
handler\_rec}s, and the last is a {\tt module}. Surround them all with {\tt
\#pragma}s as follows:

\begin{verbatim}
static const handler_rec mod_gzip_handlers[] =
{
    {"mod_gzip_handler", mod_gzip_handler},
    {CGI_MAGIC_TYPE,     mod_gzip_handler},
    {"cgi-script",       mod_gzip_handler},
    {"*",                mod_gzip_handler},
    {NULL}
};
\end{verbatim}

becomes:

\begin{verbatim}
#pragma box(off)
static const handler_rec mod_gzip_handlers[] =
{
    {"mod_gzip_handler", mod_gzip_handler},
    {CGI_MAGIC_TYPE,     mod_gzip_handler},
    {"cgi-script",       mod_gzip_handler},
    {"*",                mod_gzip_handler},
    {NULL}
};
#pragma box(on)
\end{verbatim}

Finally, change the declaration of the global module to use the new
specialized type we created earlier. For example, change:

\begin{verbatim}
module MODULE_VAR_EXPORT urlcount_module = {
\end{verbatim}

into:

\begin{verbatim}
module_urlcount MODULE_VAR_EXPORT urlcount_module = {
\end{verbatim}

Voil\`a. 

\section{Linking Apache Modules}

Suppose you have just finished making the source modifications to {\tt
mod\_foo.c}. Now you want to test it on Apache. Use CCured to compile it to
{\tt mod\_foo.o}. Make sure that there are no {\tt WILD} pointers and that 
the sizes of types involved in the apache-module interface did not change. 
Now you must link it:

\begin{verbatim}
    $ gcc -shared -o mod_foo.so mod_foo.o
    $ cp mod_foo.so /path/to/apache/bin/
\end{verbatim}

Now go to your Apache binary directory and edit {\tt httpd.conf}. Go to the
{\tt LoadModule} section and add something appropriate according to the
documentation for your module. For example, {\tt mod\_usertrack} can be
configured by adding:

\begin{verbatim}
LoadModule usertrack_module bin/mod_usertrack.so
CookieTracking On
CookieExpires "1 days"
\end{verbatim}

Now try to start Apache:

\begin{verbatim}
    $ ./apachectl stop
    $ ./apachectl start
    ./apachectl start: httpd started
\end{verbatim}

If you see the ``{\tt httpd started}'' message, it worked. If there were
messages about undefined symbols, you probably have to write a few
wrappers. For example, you might see:

\begin{verbatim}
    ./httpd: undefined symbol strdup_ff: mod_foo cannot be loaded
\end{verbatim}

In this case you must write a wrapper for {\tt strdup} that uses {\tt FSEQ}
pointers. Suppose you write it in {\tt wrapper\_foo.c} and compile that to
{\tt wrapper\_foo.o}. Now go back to the linking step: 

\begin{verbatim}
    $ gcc -shared -o mod_foo.so mod_foo.o wrapper_foo.o
    $ cp mod_foo.so /path/to/apache/bin/
\end{verbatim}

And try to start Apache again. Eventually this process converges (you can
skip ahead by using a utility like {\tt nm} to list all of the undefined
symbols in {\tt mod\_foo.so} if you like) and your Apache module will be up
and running.

If for some reasons your Apache module crashes at run-time, consider using
the underlying CIL {\tt --logcalls} mechanism to track down the error
(Apache modules do not treat well with normal debuggers). Make sure that
the debugging comments are directed to {\tt syslog(3)} rather than {\tt
printf(3)} or somesuch. 

As daunting as it may seem, it actually takes less than 30 minutes to Cure
an Apache module of average size and get it up and running. Some of that
time is spend reading the module's documentation so that it can be loaded
and tested correctly. Good luck!

\chapter{Controlling CCured}

 There are three ways to control the operation of CCured: modify the cured
code, insert type annotations, insert pragmas. 
 
  \section{CCured Type Annotations}

 CCured tries to infer an appropriate kind for a pointer. You have the choice
to specify such kinds yourself, as attributes. Typically an attribute for a
pointer goes immediately after the \t{*} symbol:
\begin{verbatim}
int * __FSEQ * __SAFE safe_pointer_to_forward_sequence_pointer_to_int;
\end{verbatim}

 The following kinds of pointers can be used:
\begin{itemize}
\item \t{\_\_SAFE} - A safe pointer represented as one word.
\item \t{\_\_SEQ} - A sequence pointer represented as three words (the pointer,
the start and the end of the home area). Such a pointer can be involved in
pointer arithmetic.
\item \t{\_\_FSEQ} - A forward sequence pointer represented as two words (the
pointers, and the end of the home area). Such a pointer can only be moved
forward by pointer arithmetic. This is the most frequent form of a forward
sequence pointer. 
\item \t{\_\_INDEX} - A sequence pointer represented as two words (the pointer
and the start of the home area). The home area for such a pointer must store
the length of the home area.
\item \t{\_\_WILD} - A wild pointer that is represented as two words (the
pointer and the start of a home area). These pointers can be involved in
arbitrary casts. 
\end{itemize}

  \section{CCured Pragmas}\label{sec-pragma}
 
 The following pragmas are recognized by CCured. Note that pragmas can only
appear in between global declarations. Some of them are discussed in more
detail in following sections:
\begin{enumerate}
\item \t{#pragma box(off)} - Turn off curing
\item \t{#pragma box(on)} - Turn curing back on
\item \t{#pragma nobox("myfunc")} - Turn curing off for the function
\t{myfunc} 
\item \t{#pragma boxtext(...)} - CCured will turn this pragma into the \t{...}
text in the cured file
\item \t{#pragma boxpoly("myfunc")} - CCured will treat the \t{myfunc}
function polymorphically. See \secref{poly}.
\item \t{#pragma boxalloc("malloc", nozero, sizein(1))} - CCured will treat
\t{malloc} as an allocation function whose length is passed in the first
argument and which does not zero the allocated area.  See \secref{malloc}.
\item \t{#pragma boxalloc("calloc", zero, sizemul(1,2))} - CCured will treat
\t{calloc} as an allocation function whose length is passed are the product of
the first two arguments and which does zero the allocated area.   See
\secref{malloc}.
\item \t{#pragma boxvararg("myfunc", sizeof(union myfunc\_arguments))} -
Declares \t{myfunc} to be a variable argument function that can be passed a
variable number of arguments each having one of the types of the fields of
\t{union myfunc\_arguments}. See \secref{vararg}.
\item \t{#pragma boxvararg\_printf("myprintf", 2)} - Declares \t{myprintf} to
be a printf-like function whose format string is in the second argument.
 See \secref{vararg}.
\item \t{#pragma cilnoremove("func1", "var2", "type foo", "struct bar")} -
Instructs CIL to keep the declarations and definitions of the function
\t{func1} and variable \t{var2}, the definition of type \t{foo} and of
structure \t{bar}.
\end{enumerate}

 
  \section{Polymorphic functions}\label{sec-poly}

 C programmers use \t{void *} to implement parametric polymorphism. Take for
example the simple \t{identity} function defined below:

\begin{verbatim}
void* identity(void *x) { return x; }
\end{verbatim}

 If we use this function multiple times with incompatible types then CCured
will be forced to infer the \t{DYNAMIC} type for the type of the argument
\t{x}. Alternatively the following pragma can be used {\bf before the first
use of \t{identity}}. 

\begin{verbatim}
#pragma boxpoly("identity")
\end{verbatim}
 
 Upon seeing this pragma CCured will pretend that each call site of
\t{identity} calls a distinct function. In fact, if \t{identity} is also
defined in the project, CCured will create a copy of its body for each
invocation. This will allow the type inferencer to infer appropriate types for
each call site. After inference, CCured coalesces those copies that have the
same adjusted type.

 Notice that if the body of the function refers to globals, then all copies
will refer to the same global. This includes functions. An exception is made
when the body of the polymorphic function calls another polymorphic function
(or itself). In this case the call site is changed to use a new instance of
the called function, for which recursively we will make a copy of the body as
well. For a recursive function, the call site is changed to use the copy of
the function that is being created, thus preserving the recursion and ensuring
the termination of the process. Note that you can construct a program whose
size will grow exponentially if you have a chain of polymorphic functions that
call each other several times. 

  \section{User-defined memory allocators}\label{sec-malloc}

 If your program has a user-defined memory allocator which is used to allocate
multiple kinds of pointers then its return type will be \t{WILD} and so will
be all of the pointers you allocate with it. Declaring such a function to be
polymorphic will likely not help because the function is probably using a
global data structure (the allocation buffer) that is shared by all
polymorphic copies of the function. 

 CCured allows you to declare a function to be a user-defined memory allocator
using one of the following pragmas:
\begin{verbatim}
#pragma boxalloc("myfunc", <zerospec>, <sizespec>)
<zerospec> ::= zero | nozero
<sizespec> ::= sizein(k) | sizemul(k1, k2)
\end{verbatim}

 The \t{zero} argument means that the allocator zeroes the allocated area. The
\t{sizein(k)} argument means that the allocator is passed the size (in bytes)
of the area to be allocated in argument number $k$ (couting starts at 1). The
\t{sizemul(k1, k2)} argument means that the allocator allocates a number of
bytes equal to the product of the arguments number $k1$ and $k2$. 

 For example the following are the pragmas for the standard library allocators
\t{malloc} and \t{calloc}:
\begin{verbatim}
void* malloc(unsigned int size);
#pragma boxalloc("malloc", nozero, sizein(1))
void* calloc(unsigned int nr_elems, unsigned int size);
#pragma boxalloc("calloc", zero, sizemul(1, 2))
\end{verbatim}

 Note that declaring a function an allocator has the effect of also making it
polymorphic and its definition is excluded from the cure. 

  \section{Models for external functions}\label{sec-model}

 When CCured handles an external function it does not assume anything about
its behavior. A good example is the \t{fgets} function from the C library:
\begin{verbatim}
char* fgets(char* buff, int size, FILE *f);
\end{verbatim}

 This function returns the exact character buffer that was passed in. However,
will not see any connection between the return value and the \t{buff} argument
and might legitimately infer incompatible types for them, such as \t{SAFE} and
\t{DYNAMIC}. To prevent this, the programmer can declare a model for the
function as follows:

\begin{verbatim}
static inline
char *fgets_model(char *buff, int size, FILE *f) __BOXMODEL("fgets");
static inline
char *fgets_model(char *buff, int size, FILE *f) {
     return buff;
}
\end{verbatim}

 (The specifiers \t{static} and \t{inline} are recommended if the above code
 is placed in an include file. The function has a separate prototype because
 \t{gcc} does not allow function attributes to be associated with function
 definitions.)

 Functions that are defined cannot have models. One model can have several
 \t{\_\_BOXMODEL} attributes. 

 For each function with a model CCured creates a dummy body that just invokes
 the model, as follows: 

\begin{verbatim}
char *fgets(char *buff, int size, FILE *f) {
    return fgets_model(buff, size, f);
}
\end{verbatim}

 This will eventually ensure the proper connection between the \t{buff}
argument and the return value. 

 In the model you can use the function \t{\_\_endof} applied to a pointer to
specify that the pointer's representation must be one that allows the
computation of the end of the home area of the pointer. Similarly, use
\t{\_\_startof} to say that you want to be able to computer the start of the
home area. 

 The dummy bodies are removed after type inference. 

 The model can contain any code. In fact, the best model would the the code of
 the function itself, but typically one much smalled suffices.

     \subsection{Polymorphic models}

 For a polymorphic function you should have polymorphic models (use the
\t{\#pragma boxpoly} described above. A separate dummy body is created {\bf for
each invocation of the modeled function}. When these dummy bodies are
processed the calls to the polymorphic model lead to new instances of the
model function. 
 

       \section{Variable argument functions}\label{sec-vararg}

 CCured supports variable-argument functions written using the \t{<stdarg.h>}
macros. There are two kinds of variable-argument functions in C: 
\begin{itemize}
\item Those that take an arbitrary number of arguments following the last
specified formal (their function type contains \t{...} after the last formal).
We'll call these functions vararg functions. An example is \t{printf}:
\begin{verbatim}
int printf(const char* format, ...)
\end{verbatim}

\item Those that take as arguments one or more pointers to lists of
arguments. We'll call these functions valist functions. An example is
\t{vprintf}:
\begin{verbatim}
int vprintf(const char* format, va_list args)
\end{verbatim}
\end{itemize}

 To allow CCured to process calls to and bodies of variable-argument functions
the programmer must specify a comprehensive list of all the types of arguments
that can be passed to each such function. This is done by defining a \t{union}
data type whose fields have the types that can be passed to the function. The
order and the names of the fields do not matter. For example, such a union for
\t{printf} would be the following:
\begin{verbatim}
union printf_arguments {
   int      f_int;
   double   f_double;
   char    *f_string;
};
\end{verbatim}

 Then we must specify for each local and formal of type \t{va\_list} which is
the union that describes the possible argument types: 
\begin{verbatim}
va_list __BOXVARARG(union printf_arguments) args1;
\end{verbatim}

 This method does not work for vararg functions because there is no formal
standing for the \t{...}. To solve this problem, and as an alternative
specification for all variable argument functions, one can use the following
pragma:
\begin{verbatim}
#pragma boxvararg("myvarargfunction", sizeof(union printf_arguments))
\end{verbatim}

 to associate with {\bf all} of the \t{va\_list} formals and locals in the
function \t{myvarargfunction} the given union. (The \t{sizeof} operator is
there because the syntax of pragmas is pretty much that of function calls.)
{\bf The pragma must appear before the definition and any invocation of the
function.} An equivalent method is to associate the \t{\_\_BOXVARARG(union
printf\_arguments)} attribute with the type of the function
\t{myvarargfunction}:
\begin{verbatim}
int (__BOXVARARG(union printf_arguments) myvarargfunction)(int last, ...);
\end{verbatim}

 In vararg functions, the macro \t{va\_start} is used to initialize an
\t{va\_list} variable to point to the trailing arguments. CCured checks that
this macro is used with a \t{va\_list} variable with the same \t{union} type
as the host function itself, and also checks that the second argument is the
last formal before the \t{...}. 

 Both in vararg and valist functions the macro \t{va\_arg} can be used, as
 follows: 
\begin{verbatim}
 T x = va_arg(args, T)
\end{verbatim}

 \t{args} must be a \t{va\_list} variable and \t{T} must be compatible after
the usual argument promotions (e.g. \t{char} and \t{short} to \t{int} and
\t{float} to \t{double}) with one of the types in the \t{union} associated
with \t{args}. CCured checks this at run-time. 

 The CCured support for variable argument functions is quite flexible.
Multiple variable argument lists can be processed in parallel, an argument
list can be re-initialized with \t{va\_start} and processed multiple times. A
function can even work with variable argument lists that have different sets
of types accepted. Variable argument lists can be passed down but the regular
CCured checks for stack allocated variables will prevent the passing of these
lists up the call chain and also their storing in the heap.

 The main thing that is not supported in CCured is the fetching of an argument
with a different type than it was stored. It remains to be seen if this is a
problem. We have looked at several variable argument functions (including
implementations of \t{printf} and \t{sprintf}) and so far we have found that
CCured accepts those functions without any change except for the specification
of the \t{union} of the accepted argument types. 

           \subsection{Implementation Issues}

 Almost all of the checking for variable-argument functions is done at
run-time. At the time of a call each actual argument is compared with the
types in the \t{union} associated with the vararg function. A global data
structure is filled with the number of arguments and a list of indices
describing for each actual argument the index within the \t{union} types. 

 In the body of a vararg function, a data structure is allocated on the stack
to hold a copy of the global description of the arguments that was created by
the caller. The call to \t{va\_start} initializes this data structure and each
call to \t{va\_arg} checks that we are are not reading past the end of the
actuals and also that the type of the fetched argument matches that of the
actual argument. 

         \subsection{Printf-like functions}

 Since the vast majority of uses of variable argument functions if for
\t{printf}-like functions, CCured contains special support for them.
Specifically if a vararg function is declared to be a \t{printf}-like function
then all of its invocations in which the format string is a constant will be
checked statically. For the other invocations a wrapper for printf will be
called that will check the types of the actuals agains the format string
before calling the real \t{printf} function.

 To declare a function to be \t{printf}-like use the following pragma:
\begin{verbatim}
#pragma boxvararg_printf("myprintf", 1)
\end{verbatim}

 where the last argument is the index of the format argument in the argument
list (starting with 1). Note that you will get a run-time error if you try to
use the \t{va\_arg} macro in the implemetation of such a function. In those
implementations you should invoke functions like \t{vprintf} and \t{vsprintf}
instead.

 GCC already has support for communicating to the compiler that a function is
\t{printf}-like. This is done as follows:
\begin{verbatim}
int myprintf(const char* format, ...) __attribute__(format(printf, 1, 2))
\end{verbatim}
 
 where the ``1'' means that the first argument is the format string and the
``2'' means that we should start checking with the second argument. CCured
recognizes this attribute and it considers it equivalent with the
\t{boxvararg\_printf} from above. Note that the second argument in the
\t{format} attribute is ignored. 

 Note that CCured does not currently like passing pointers to \t{printf} with
the intention of printing the pointer value. You should manually cast those
pointers to \t{long} when passing them to \t{printf}-like functions.

    \subsection{Scanf-like functions}

 Since it proved too much trouble to handle \t{scanf}-like functions in a safe
yet transparent way we currently require the programmer to rewrite the
invocations to \t{scanf} using a number of functions that we provide. For
example instead of 
\begin{verbatim}
 ... fscanf(file, "Entry:\%d   Then:\%lf", &entry, &then) ...
\end{verbatim}

 you should write
\begin{verbatim}
 ... (resetScanfCount(), 
      entry = ccured_fscanf_int(file, "Entry:\%d"),
      then  = ccured_fscanf_double(file, "   Then:\%lf"),
      getScanfCount ()) ...
\end{verbatim}

 The functions \t{resetScanCount} and \t{getScanfCount} are necessary only
if you use the result of the call to \t{fscanf} in the original code. Note
that our replacement \t{scanf} functions can be used to return only one result
at a time, consequently the format string that is passed must contain only one
format specifier, possibly along with characters to be matched. 

 The following are the \t{scanf}-like functions that we currently support:
\begin{verbatim}
  extern int    ccured_fscanf_int(FILE *, char *format);
  #pragma boxpoly("ccured_fscanf_int")
  extern double ccured_fscanf_double(FILE *, char *format);
  #pragma boxpoly("ccured_fscanf_double")
  extern void   ccured_fscanf_nothing(FILE *, char *format);
  #pragma boxpoly("ccured_fscand_nothing")
\end{verbatim}

 If the original program uses \t{scanf}, just consider that you are using
\t{fscanf} from \t{stdin}. If instead your program contains \t{sscanf} then
you can use the function 
\begin{verbatim}
void resetSScanfCount(char *string);
\end{verbatim}

 to dump the string to the file \t{ccured\_sscanf\_file} then use the
replacement for \t{fscanf} from above. 

 {\bf Note that the current support for \t{scanf} is far from satisfactory and
 will likely change in the future}


    \chapter{Using the patcher}\label{ch-patcher}

 Occasionally for the purposes of CCured we have needed to modify slightly the
standard include files. So, we developed a simple mechanism that allows us to
create modified copies of the include files and use them instead of the
standard ones. For this purpose we specify a patch file and we run a program
caller that Patcher which makes modified copies of include files and applies
the patch. 

 The patcher is invoked as follows: 
\begin{verbatim}
perl lib/patcher.pl [options]

Options:
  --help       Prints this help message
  --verbose    Prints a lot of information about what is being done
  --mode=xxx   What tool to emulate: 
                gcc     - GNU CC
                mscl    - MS VC cl compiler

  --dest=xxx   The destination directory. Will make one if it does not exist
  --patch=xxx  Patch file (can be specified multiple times)
  --ppargs=xxx An argument to be passed to the preprocessor (can be specified
               multiple times)

  --ufile=xxx  A user-include file to be patched (treated as \#include "xxx")
  --sfile=xxx  A system-include file to be patched (treated as \#include <xxx>)
 
  --clean       Remove all files in the destination directory
  --dumpversion Print the version name used for the current compiler

 All of the other arguments are passed to the preprocessor.
\end{verbatim}

 Based on the given \t{mode} and the current version of the compiler (which
the patcher can print when given the \t{dumpversion} argument) the patcher
will create a subdirectory of the \t{dest} directory (say \t{/usr/home/necula/cil/include}), such as:
\begin{verbatim}
/usr/home/necula/cil/include/gcc_2.95.3-5
\end{verbatim}

 In that file the patcher will copy the modified versions of the include files
specified with the \t{ufile} and \t{sfile} options. Each of these options can
be specified multiple times. 

 The patch file (specified with the \t{patch} option) has a format inspired by
the Unix \t{patch} tool. The file has the following grammar:

\begin{verbatim}
<<<
patterns
===
replacement
>>>
\end{verbatim}

 The patterns can consist of several groups of lines separated by the \t{|||}
marker. Each of these group of lines is a multi-line pattern that if found in
the file will be replaced with the text given at the end of the block. 

 The matching is space-insenstive.

 All of the markers \t{<<<}, \t{|||}, \t{===} and \t{>>>} must appear at the
beginning of a line but they can be followed by arbitrary text (which is
ignored).

 The replacement text can contain the special keyword \t{@\_\_pattern\_\_@},
which is substituted with the pattern that matched. 

  \chapter{Using the combiner}\label{ch-combiner}

 The easiest way to use CCured is when you want to process a whole program and
thus allow the inferencer to see all of the uses of all defined functions and
variables. This way CCured is able to infer the best representation for such
globals. There are many other program analyses that are more effective when
done on the whole program.

 The combiner is a tool that combines all of the C source files in a project
into a single C file. There are two tasks that a combiner must solve:
\begin{enumerate}
\item Detect what are all the sources that make a project and with what
compiler arguments they are compiled.

\item Combine all of the source files into a single file. 
\end{enumerate}

 For the first task the combiner impersonates a compiler and a linker (both a
GCC and a Microsoft Visual C mode are supported) and it expect to be invoked
(from a build script or a Makefile) on all sources of the project. When
invoked to compile a source the combiner just preprocesses the source and
saves using the name of the requested object file. By preprocessing this early
the combiner is able to take into account variations in the command line
arguments that affect preprocessing of different source files. 

 When the combiner is invoked in the place of the linker it collects the
preprocessed sources that were stored with the names of the object files, and
invoked the second part of the combiner. Note that arguments that affect the
compilation or linking must be the same for all source files.

 For the second task, the combiner essentially concatenates the preprocessed
sources with care to rename conflicting file-local declarations (we call this
process alpha-conversion or a file). The combiner also attempts to remove
duplicate global declarations and definitions. Specifically the following
actions are taken: 

\begin{itemize}
\item File-scope names (\t{static} globals, names of types defined with
\t{typedef}, and structure/union/enumeration tags) are given new names if they
conflict with declarations from previously processed sources. The new name is
formed by appending the suffix \t{\_\_\_n}, where \t{n} is a unique integer
identifier. Then the new names are applied to their occurences in the file. 

\item Non-static declarations of globals are never renamed. But we try to
remove duplicate ones. The equality check is done on the whole structure of
the declaration (including the line-number information) after the body of the
declaration has been alpha-converted. This process is intended to remove those
declarations that originate from the same include file. Similarly, we try to
eliminate duplicate definitions of \t{inline} functions, since these
occasionally appear in include files.

\item Names of types and tags of structures or unions or enumerations are
considered to have file scope and thus are candidate for renaming. However, if
we detect an existing declaration with the same body from a previously
processed file, we reuse it.

\item In rare situations, it can happen that a file-local global in
encountered first and it is not renamed, only to discover later when
processing another file that there is an external symbol with the same name.
In this case, a second pass is made over the combined file to rename the
file-local symbol. 
\end{itemize}

\chapter{Using the Regression Tester}\label{ch-regtest}

 The regression tester is a program that allows you to do two things: 
\begin{itemize}
\item Run a list of shell commands and capture their standard and error output
 in a log file, and

\item Analyze such log files and extract various information, among which the
 most important is which test cases have succeeded and which have failed. 
\end{itemize}

 Since the running of the tests and the analysis of the output is separated
you can easily do things like compare the results on multiple runs, extract
various reports from a single output (like what tests have succeeded, which
have failed, plus such information split by test groups). You can also extract
some data from each test (such as the running time) and make simple reports. 

 Test cases can have comments associated with them (such as reminders of why
it fails) and can be associated with zero or more test groups.

 \section{Running the regression}

 The regression tester is implemented in Perl as "\t{testsafec.pl}", which in
turn contains simple wrappers for functions provided by the more generic
\t{RegTest.pm}.

 The regression tester uses relative paths so it must be run in the
 \t{cil/test} directory.

 The basic command for running the tests is "\t{testsafec --run}". This runs
all of the test cases, saving the log in the file "\t{safec.log}". Before
creating this file, it renames previous versions of this file as
"\t{safec.log.<n>}" where n is an integer starting from 1 to a maximum number
that is configurable.

 The following command line options are useful for running the tests (see
 "\t{testsafec --help}" for a complete list:

\begin{verbatim}
 --one <testname>       : runs only the named test
 --gory                 : shows lots of details about the execution of the
                          test, such as the commands executed
 --dryrun               : only pretends to run the test. Useful to see what
                          would be run
 --log                  : select the base name of the log file (default
                          "safec.log")
 --logversions <n>      : keep logs up to version <n>. Default is 5.
 --noremake             : runs the commands without trying to remake the safec
                          compiler before each test. Useful if you want to 
                          work on the compiler while the tests are running
 --safecdebug           : uses the DEBUG version of the safec compiler and
                          uses the C compiler in debug mode. By default it
                          used the RELEASE version and the optimizing compiler.



 --group <groupname>    : adds all the tests in the named group to the list of 
                          tests to be run or to participate in the analysis of
                          the log. (Right now we have groups: apache,
                          bad, cil, box, infer.) If no such option is
                          specified then all tests are selected. Multiple such
                          options can be given and are cumulative. 
 --nogroup <groupname>  : excludes the tests in the named group from running
                          or from the analysis. Multiple such options can be
                          given and are cumulative. These options are
                          processed after all --group options have 
                          been processed. 

 --listtests            : list the tests that are enabled along with their
                          group membership. This is useful to find out what
                          tests and groups exist.
\end{verbatim}

 \section{Analysing the results}

 The basic command for analyzing log files is "\t{testsafec}". This will
prompt the user to select one of the several log files that exist in the
current directory and then (by default) it will print a list of the failed
test cases, with a short (user provided) comment and the last error message
detected in the output for that test case.

 The following commands are useful during analysis:
\begin{verbatim}
 --log                  : select the log file (see above)
 --group, --nogroup     : select groups (see above)
 --listtests            : list tests and groups (see above)
 --failures             : show the failures (default)
 --successes            : show the successes (they are not shown by defauls)
 --param=<pnames>       : show a report about the successes, with the columns
                          being the named parameters (separated by ,). Run
                          "testsafec --help" to see what parameters are
                          available. 
 --sort=<pnames>        : sorts the report by the given parameters. 
\end{verbatim}

 
 \section{Configuring the regression}


 For this you have to edit testsafec.pl. You will see a large section in the
 middle of the file containing lines like:

\begin{verbatim}
\$TEST->add3Tests("test/array1");
\end{verbatim}


 (check out the definintion of \t{add3Tests} at the bottom of the file). This
 adds three tests named "\t{test/array1-cil}", "\t{test/array1-box}" and
 \t{test/array1-inferbox}", each one containing one command that invokes
 "\t{make test/array1 ...}", where \t{...} are appropriate parameters.

 A second optional string parameter to \t{add2Tests} is something to be added
 to the command line.

 A third optional array parameters is a list of patterns to be used in
scanning the output of the test cases. This is an advanced feature and you are
on your own. 


 To add just one test do (as in the body of add3Tests):

\begin{verbatim}
    $TEST->newTest(Name => "mytestname",
                   Dir => "..",
                   Cmd => "make something",
                   Enabled => 1,
                   Comm => "Print this along with the test name",
                   Group => ["cil", "othergroup"],
                   Patterns => \%mypatterns);
\end{verbatim} % $
 Sometimes you might want to add just a comment or to add one group to a
 certain test. Use the following simple functions: 

\begin{verbatim}
 $TEST->addGroups("mytestname", "group1", "group2");
 $TEST->addComment("mytestname", "Another line of comment");
\end{verbatim}

 There are soem wrappers defined at the end of testsafec.pl:

\begin{verbatim}
   $TEST->add3BadComment("test/scope3", "missing prototype");
\end{verbatim}%$

 (this one adds a comment and the group "bad" to all three test cases)

\begin{verbatim}
  $TEST->addBadComment("li-box", "bug in box.ml");
\end{verbatim}%$

 (the same but just for one test)
   
\begin{verbatim}
  $TEST->enable("li-box", 0);   (disable the li-box test case)
\end{verbatim}%$


 For more advanced customization, read the Perl code. It is fairly easy to
 understand, especially the testsafec.pl. 

 \section{A simpler regression tester}
        
 Scott has implemented a simpler regression tester. You can use it if you want
 but you should also make sure that whenever you make changes the main
 regression test passes. To use Scott's regression tester:

\begin{verbatim}
  % cd cil
  % ./regrtest
\end{verbatim}

This should run to completion and report something like:

\begin{verbatim}
  All 93 regression tests passed!
  11 tests failed as expected
\end{verbatim}

If a particular test fails (or unexpectedly succeeds), you can tell
the regression tester to skip past that one to proceed with the rest.
For example, if it stops like this:

\begin{verbatim}
  [74] A regression test command failed:
    make test/attr4 INFERBOX=4
\end{verbatim}

then it failed on the 75th test (they're numbered from 0), so restart
the script with:

\begin{verbatim}
  % ./regrtest -skip 75
\end{verbatim}

And of course, report these to us!


        \chapter{Getting Started}\label{ch-start}

 CCured works on Linux and MS Windows (Win95 operation is unreliable but Win98
and (highly recommended) Win2k should work). CCured might also work on other
systems that use \t{gcc}, but we have not tried it.

 If you use Windows you will need cygwin, cvs, ssh, perl and a version of
ocaml built for cygwin. Otherwise you probably only need to get and build
ocaml and configure ssh. For this, please follow the directions at

\ahrefurl{setup.html}

\section{Get the C-Cured sources via CVS}

You'll need CVS (Concurrent Version System) installed; version 1.10 or
greater is recommended.  If you don't have it (try "which cvs"), you can
get it from

  \ahrefurl{http://www.cvshome.org/}

Next you'll need an account on brooksie.cs.berkeley.edu; talk to Rahul
(\mailto{sprahul@cs.berkeley.edu}) to get an account.

Next, configure your cvs to use 'ssh' as your connection method.  If you're
using csh/tcsh, use

\begin{verbatim}
  setenv CVS_RSH ssh
\end{verbatim}

  
and for sh/bash, use

\begin{verbatim}
  export CVS_RSH=ssh
\end{verbatim}

This should probably be put into your .cshrc or .bashrc so it gets run
every time the shell starts.

Now, checkout the repository.  As an example, here's how I do it:


\begin{verbatim}
  % cvs -d :ext:smcpeak@brooksie.cs:/home/cvs-repository checkout cil
                ^^^^^^^
         use your username instead
\end{verbatim}

This will checkout all the sources into a new directory called cil/, off
the current directory.


\section{Set your environment variables}

 Put the \t{cil/lib} directory in your PATH.

 Set the \t{ARCHOS} variable to either \t{x86\_WIN32} or \t{x86\_LINUX}.

 You must set the environment variable \t{CCUREDHOME} to point to the
directory in which you installed CCured. In that directory you will find a
file called \t{.ccuredrc\_model}. Make a copy of it called \t{.ccuredrc} and
place it in the same directory. Edit \t{.ccuredrc} according to your needs. If
you use \t{gcc} on Linux you typically don't need to change anything. In this
case you don't even need to have the \t{.ccuredrc} file. {\bf Never check in
\t{ccuredrc} in the CVS repository since here you will have your own
configuration options.}.

\section{Compile CCured}

Go into the cil/ directory, and run GNU make:

\begin{verbatim}
  % cd cil
  % gmake setup       (or maybe just 'make setup')
\end{verbatim}

\section{Run CCured}

Go into \t{cil} and run
\begin{verbatim}
 make testrun/hello INFERBOX=infer
\end{verbatim}


\appendix



\chapter{A Tour of the Source Code}
\begin{verbatim}
./: Makefile: the top level Makefile. This contains mostly test cases
    Makefile.ccured: instructions for building CCured
    Makefile.ocaml: used by Makefile.ccured for the Ocaml part
    Makefile.gcc: Included in the above if you use gcc
    Makefile.msvc: Included in the above if you use Microsoft Visual C

./src/: (ML code)
  box: insert runtime checks
  check: a consistency checker for CIL. The most precise "documentation" of
  the meaning of CIL.
  cil: representation of intermediate lang (C but no side
       effects in expressions); includes cil -> doc
  cilparse: parsing our stuff?
  errormsg: A few utilities for reporting error messages and for logging
  frontc: C parser (C source -> cabs -> cil)
  globinit: add initializers to modules?
  main: entry point to 'safec' tool; processes command-line flags
  markptr: marks pointers based on usage (input to inferencer)
  mllex: ?
  notes.txt: describes syntax of 'asm' statements in gcc
  oneret: a simple program transformation that pull all return statements to
  the end of the function body. A good example of how to use CIL.
  optim: rahul's bounds-check elim stuff
  pretty: generic pretty-printer (doc -> string)
  ptrnode: graph for inference algorithm
  rmtmps: remove unused temporaries introduced by 'box'
  secondsolve: 2nd solver?
  simplesolve: 1st solver; decides which pointer flavors to use
    based on how those pointers were used by programmer
  stats: a few utilities for timing the execution
  thirdsolve: 3rd solver (currently used; INFERBOX=3)
  trace: something for debugging output
  util: ?
  wildsolve: ?

./lib/: (C code for use at runtime)
  fixup.h: prepended to every input source file before boxing
  safec.c: library wrappers (e.g. fopen_w)
  safec_GNUCC.patch: patching nonsense (for GCC)
  safec_MSVC.patch: patching nonsense (for MSVC)
  safecc.pl: intended to be drop-in replacement for 'gcc' and for MS 'cl'
  safeccheck.h: "inline" macros for doing runtime checks
  safecmain.c: wrapper for 'main' in case it takes arguments
  scaninfer: ?
  gc/: boehm-weiser garbage collector (http://www.hpl.hp.com/personal/Hans_Boehm/gc/)
\end{verbatim}

 

\chapter{Using attributes with CIL}

 In CIL you can attach attributes to types and to names (variables, functions
and fields). The following syntax of attributes is currently supported: 

\begin{verbatim}
 attribute ::= IDENT | IDENT ( [attrarg ,]+ )
 attrarg   ::= IDENT | IDENT ( [attrarg ,]+) | "STRING" | INT
             | attrarg binop attrarg | unop attrarg
             | sizeof attrarg | sizeof type
\end{verbatim}

 The attribute names (IDENT above) must start with a letter and should not
start or end with underscore.

 The attributes are specified in declarations. This is unfortunate since the C
syntax for declarations is already quite complicated and after writing the
parser and elaborator for declarations I am convinced that few C programmers
understand it completely. Anyway, this seems to be the easiest way to support
attributes. 

 Name attributes must be specified at the very end of the declaration, just
before the = for the initializer or before the , the separates a declaration
in a group of declarations or just before the ; that terminates the
declaration. A name attribute for a function being defined can be specified
just before the brace that starts the function body. 

 For example (in the following examples A1,...,An are type attributes and N
  is a name attribute. We'll talk soon about how these attributes can be
  written in the source program): 

\begin{verbatim}
 int x N1;
 int x Nx, * y Ny = 0, z[] Nz;
 extern void exit() N;
 int fact(int x) N { ... }
\end{verbatim}


 Type attributes can be specified along with the type using the following
 rules: 
\begin{enumerate}
 \item The type attributes for a base type (int, float, named type, reference
    to struct or union or enum) must be specified immediately following the
    type (actually it is Ok to mix attributes with the specification of the
    type, in between unsigned and int for example).

  For example:
\begin{verbatim}
  int A1 x N1;  /* A1 applies to the type int. An example is an attribute
                   "even" restricting the type int to even values. */
  struct foo A1 A2 x; // Both A1 and A2 apply to the struct foo type
\end{verbatim}
 
 \item The type attributes for a pointer type must be specified immediately
 after the * symbol.
\begin{verbatim}
 /* A pointer (A1) to an int (A2) */
 int A2 * A1 x;
 /* A pointer (A1) to a pointer (A2) to a float (A3) */
 float A3 * A2 * A1 x;
\end{verbatim}


 Note: The attributes for base types and for pointer types are a strict
 extension of the ANSI C type qualifiers (const, volatile and restrict). In
 fact the is special support to parse these qualifiers as attributes. 

  \item The attributes for a function type or for an array type can be
     specified using parenthesized declarators.

   For example:
\begin{verbatim}
   /* A function (A1) from int (A2) to float (A3) */
   float A3 (A1 f)(int A2);

   /* An array (A1) of int (A2) */
   int A2 (A1 x0)[]

   /* Array (A1) of pointers (A2) to functions (A3) that take an int (A4) and 
    * return a pointer (A5) to int (A6)  */
   int A6 * A5 (A3 * A2 (A1 x1)[5])(int A4);


   /* A function (A4) that takes a float (A5) and returns a pointer (A6) to an 
    * int (A7) */
   extern int A7 * A6 (A4 x2)(float A5 x);

   /* A function (A1) that takes a int (A2) and that returns a pointer (A3) to 
    * a function (A4) that takes a float (A5) and returns a pointer (A6) to an 
    * int (A7) */
   int A7 * A6 (A4 * A3 (A1 x3)(int A2 x))(float A5) {
      return & x2;
   }
\end{verbatim}

\end{enumerate}

 Note: ANSI C does not allow the specification of type qualifiers for function
and array types, although it allows for the parenthesized declarator. With
just a bit of thought (looking at the first few examples above) I hope that
the placement of attributes for function and array types will seem intuitive.

 This extension is not without problems however. If you want to refer just to
a type (in a cast for example) then you leave the name out. But this leads to
strange conflicts due to the parentheses that we introduce to scope the
attributes. Take for example the type of x0 from above. It should be written
as: 
 
\begin{verbatim}
        int A2 (A1 )[]
\end{verbatim}

 But this will lead most C parsers into deep confusion because the parentheses
around A1 will be confused for parentheses of a function designator. To push
this problem around (I don't know a solution) whenever we are about to print a
parenthesized declarator with no name but with attributes, we comment out the
attributes so you can see them (for whatever is worth) without confusing the
compiler. For example, here is how we would print the above type:

\begin{verbatim}
        int A2 /*(A1 )*/[]
\end{verbatim}

 
 \section{Source-level representation of attributes}

 GCC already has extensive support for attributes, so we are going to extend
it to handle arbitrary attributes. A GCC attribute has the syntax:
 
\begin{verbatim}
 gccattribute ::= __attribute__((attribute))    (Note the double parentheses)
\end{verbatim}

 Since GCC and MSVC both support various flavors of each attribute (with or
without leading or trailing \_) we first strip ALL leading and trailing \_ from
the attribute name (the IDENT in the non-terminal "attribute", but not the
IDENT in the attribute arguments "attrarg"). When we print attributes, for GCC
we add two leading and two trailing \_; for MSVC we add just two leading \_.
 
 There is support in CIL so that you can control the printing of attributes.
This custom-printing support is now used to print the "const" qualifier as
"\t{const}" and not "\t{\_\_attribute\_\_((const))}". 


 \section{Handling of predefined GCC attributes}

 GCC already supports attributes in a lot of places in declarations. The only
place where we support attributes and GCC does not is right before the \{ that
starts a function body. 

 GCC classifies its attributes in attributes for functions, for variables and
for types, although the latter category is only usable in definition of struct
or union types and is not nearly as powerful as the CIL type attributes. We
have made an effort to reclassify GCC attributes in name and type attributes
(they only apply for function types). Here is what we came up with:

\begin{itemize}
  \item GCC name attributes:
   
   section, constructor, destructor, unused, weak, no\_instrument\_function,
   noreturn, alias, no\_check\_memory\_usage, dllinport, dllexport, exception,
   model

      Note: the "noreturn" attribute would be more appropriately qualified as a
      function type attribute. But we classify it as a name attribute to make
      it easier to support a similarly named MSVC attribute. 
  
  \item GCC function type attributes:

    fconst (printed as "const"), format, regparm, stdcall,
    cdecl, longcall

  I was not able to completely decipher the position in which these attributes
  must go. So, the CIL elaborator knows these names and applies the following
  rules: 
  \begin{itemize}
  \item All of the name attributes that appear anywhere in the declaration are
  collected and associated with the declared name. This was easy since each
  declaration declares exactly one name.

  \item More complicated is the handling of the function type attributes, since
     there can be more than one function in a single declaration (a function
     returning a pointer to a function). Lacking any real understanding of how
     GCC handles this, I attach the function type attribute to the "nearest"
     function. This means that if a pointer to a function is "nearby" the
     attribute will be correctly associated with the function. In truth I pray
     that nobody uses declarations as that of x3 above. 
  \end{itemize}
\end{itemize}

\section{Handling of predefined MSVC attributes}

  MSVC has two kinds of attributes, declaration modifiers to be printed before
  the storage specifier using the notation "\t{\_\_declspec(...)}" and a few
  function type attributes, printed almost as our CIL function type
  attributes. 

   The following are the name attributes that are printed using
   \t{\_\_declspec} right before the storage designator of the declaration:
   thread, naked, dllimport, dllexport, noreturn


   The following are the function type attributes supported by MSVC: 
   fastcall, cdecl, stdcall

   It is not worth going into the obscure details of where MSVC accepts these
   type attributes. The parser thinks it knows these details and it pulls
   these attributes from whereever they might be placed. The important thing
   is that MSVC will accept if we print them according to the rules of the CIL
   attributes ! 
 
 
\end{document}
